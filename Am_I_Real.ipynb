{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111ea40a-9245-4007-9e46-60361160e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e71045-2e34-4438-9520-d9c30f7fe025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define functions for loading and preprocessing data\n",
    "def load_and_preprocess_data(folder_path, label, batch_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "        img = img / 255.0\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "        if len(data) == batch_size:\n",
    "            yield np.array(data), np.array(labels)\n",
    "            data = []\n",
    "            labels = []\n",
    "\n",
    "    if data:\n",
    "        yield np.array(data), np.array(labels)\n",
    "\n",
    "# Define function for loading and preprocessing video data\n",
    "def load_and_preprocess_video(folder_path, label, batch_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        frame_path = os.path.join(folder_path, filename)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        frame = cv2.resize(frame, (64, 64))\n",
    "        frame = frame / 255.0\n",
    "        data.append(frame)\n",
    "        labels.append(label)\n",
    "\n",
    "        if len(data) == batch_size:\n",
    "            yield np.array(data), np.array(labels)\n",
    "            data = []\n",
    "            labels = []\n",
    "\n",
    "    if data:\n",
    "        yield np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e655a20d-1c31-48c1-9432-a419b3574668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and preprocess image data in batches\n",
    "image_folder_path = 'dataset'\n",
    "batch_size_img = 100  # Adjust based on memory capacity\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for folder_name, label in [('Real', 0), ('Fake', 1)]:\n",
    "    folder_path = os.path.join(image_folder_path, folder_name)\n",
    "    for batch_images, batch_labels in load_and_preprocess_data(folder_path, label, batch_size_img):\n",
    "        all_images.extend(batch_images)\n",
    "        all_labels.extend(batch_labels)\n",
    "\n",
    "all_images = np.array(all_images)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84acd7ec-069d-4e92-8721-ab4e1946922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load and preprocess video data in batches\n",
    "video_folder_path = 'dataset'\n",
    "batch_size_video = 100  # Adjust based on memory capacity\n",
    "all_frames = []\n",
    "all_video_labels = []\n",
    "\n",
    "for folder_name, label in [('Real', 0), ('Fake', 1)]:\n",
    "    folder_path = os.path.join(video_folder_path, folder_name)\n",
    "    for batch_frames, batch_labels in load_and_preprocess_video(folder_path, label, batch_size_video):\n",
    "        all_frames.extend(batch_frames)\n",
    "        all_video_labels.extend(batch_labels)\n",
    "\n",
    "all_frames = np.array(all_frames)\n",
    "all_video_labels = np.array(all_video_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b70e27-35f4-47d3-9394-dbb120bb5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split data into train and test sets\n",
    "x_train_img, x_test_img, y_train_img, y_test_img = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "x_train_video, x_test_video, y_train_video, y_test_video = train_test_split(all_frames, all_video_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0786c46-4e8c-4787-8eae-2f80fab6e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define data generator with data augmentation\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8970e061-042e-4810-979d-d6f2713b1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define models\n",
    "base_model_img = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "for layer in base_model_img.layers:\n",
    "    layer.trainable = False\n",
    "x_img = Flatten()(base_model_img.output)\n",
    "x_img = Dense(256, activation='relu')(x_img)\n",
    "output_img = Dense(1, activation='sigmoid')(x_img)\n",
    "model_img = Model(inputs=base_model_img.input, outputs=output_img)\n",
    "model_img.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "base_model_video = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "for layer in base_model_video.layers:\n",
    "    layer.trainable = False\n",
    "x_video = Flatten()(base_model_video.output)\n",
    "x_video = Dense(256, activation='relu')(x_video)\n",
    "output_video = Dense(1, activation='sigmoid')(x_video)\n",
    "model_video = Model(inputs=base_model_video.input, outputs=output_video)\n",
    "model_video.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004e681b-6270-423b-98ef-dcaba060a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hinas\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 175ms/step - accuracy: 0.5439 - loss: 0.6862 - val_accuracy: 0.5098 - val_loss: 0.7237\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 166ms/step - accuracy: 0.5814 - loss: 0.6725 - val_accuracy: 0.6084 - val_loss: 0.6617\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 161ms/step - accuracy: 0.5870 - loss: 0.6707 - val_accuracy: 0.6070 - val_loss: 0.6605\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 159ms/step - accuracy: 0.5971 - loss: 0.6647 - val_accuracy: 0.5944 - val_loss: 0.6674\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 172ms/step - accuracy: 0.5929 - loss: 0.6661 - val_accuracy: 0.6089 - val_loss: 0.6633\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 165ms/step - accuracy: 0.6001 - loss: 0.6617 - val_accuracy: 0.5875 - val_loss: 0.6616\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 189ms/step - accuracy: 0.6020 - loss: 0.6611 - val_accuracy: 0.6137 - val_loss: 0.6605\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 164ms/step - accuracy: 0.6050 - loss: 0.6603 - val_accuracy: 0.5857 - val_loss: 0.6664\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 173ms/step - accuracy: 0.5983 - loss: 0.6635 - val_accuracy: 0.6125 - val_loss: 0.6514\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 159ms/step - accuracy: 0.5998 - loss: 0.6604 - val_accuracy: 0.6055 - val_loss: 0.6516\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 154ms/step - accuracy: 0.6051 - loss: 0.6581 - val_accuracy: 0.6162 - val_loss: 0.6518\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 158ms/step - accuracy: 0.6062 - loss: 0.6563 - val_accuracy: 0.5939 - val_loss: 0.6594\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 161ms/step - accuracy: 0.6075 - loss: 0.6558 - val_accuracy: 0.6164 - val_loss: 0.6554\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 154ms/step - accuracy: 0.6110 - loss: 0.6543 - val_accuracy: 0.6219 - val_loss: 0.6473\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 157ms/step - accuracy: 0.6096 - loss: 0.6531 - val_accuracy: 0.6191 - val_loss: 0.6510\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 165ms/step - accuracy: 0.6147 - loss: 0.6511 - val_accuracy: 0.6255 - val_loss: 0.6474\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 157ms/step - accuracy: 0.6076 - loss: 0.6540 - val_accuracy: 0.6223 - val_loss: 0.6480\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 155ms/step - accuracy: 0.6118 - loss: 0.6530 - val_accuracy: 0.5894 - val_loss: 0.6692\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 149ms/step - accuracy: 0.6091 - loss: 0.6570 - val_accuracy: 0.5989 - val_loss: 0.6553\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 152ms/step - accuracy: 0.6085 - loss: 0.6540 - val_accuracy: 0.6161 - val_loss: 0.6518\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 151ms/step - accuracy: 0.6126 - loss: 0.6517 - val_accuracy: 0.6251 - val_loss: 0.6423\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 154ms/step - accuracy: 0.6060 - loss: 0.6526 - val_accuracy: 0.6240 - val_loss: 0.6466\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 150ms/step - accuracy: 0.6140 - loss: 0.6516 - val_accuracy: 0.6350 - val_loss: 0.6437\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 149ms/step - accuracy: 0.6134 - loss: 0.6520 - val_accuracy: 0.6271 - val_loss: 0.6466\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 150ms/step - accuracy: 0.6117 - loss: 0.6483 - val_accuracy: 0.6071 - val_loss: 0.6494\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 156ms/step - accuracy: 0.6174 - loss: 0.6517 - val_accuracy: 0.6244 - val_loss: 0.6495\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 154ms/step - accuracy: 0.6146 - loss: 0.6502 - val_accuracy: 0.6205 - val_loss: 0.6481\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 152ms/step - accuracy: 0.6180 - loss: 0.6465 - val_accuracy: 0.6165 - val_loss: 0.6509\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 151ms/step - accuracy: 0.6148 - loss: 0.6488 - val_accuracy: 0.5959 - val_loss: 0.6529\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 150ms/step - accuracy: 0.6216 - loss: 0.6458 - val_accuracy: 0.6300 - val_loss: 0.6428\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 165ms/step - accuracy: 0.5413 - loss: 0.6879 - val_accuracy: 0.6012 - val_loss: 0.6693\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 154ms/step - accuracy: 0.5847 - loss: 0.6714 - val_accuracy: 0.6020 - val_loss: 0.6679\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 162ms/step - accuracy: 0.5905 - loss: 0.6694 - val_accuracy: 0.6114 - val_loss: 0.6586\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 153ms/step - accuracy: 0.5871 - loss: 0.6678 - val_accuracy: 0.5975 - val_loss: 0.6583\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 152ms/step - accuracy: 0.5871 - loss: 0.6679 - val_accuracy: 0.5921 - val_loss: 0.6726\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 152ms/step - accuracy: 0.5953 - loss: 0.6628 - val_accuracy: 0.6022 - val_loss: 0.6569\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 154ms/step - accuracy: 0.5945 - loss: 0.6632 - val_accuracy: 0.6122 - val_loss: 0.6551\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 154ms/step - accuracy: 0.6019 - loss: 0.6596 - val_accuracy: 0.6120 - val_loss: 0.6606\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 153ms/step - accuracy: 0.6052 - loss: 0.6601 - val_accuracy: 0.6030 - val_loss: 0.6548\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 154ms/step - accuracy: 0.5987 - loss: 0.6588 - val_accuracy: 0.6129 - val_loss: 0.6559\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 156ms/step - accuracy: 0.6072 - loss: 0.6571 - val_accuracy: 0.6201 - val_loss: 0.6496\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 155ms/step - accuracy: 0.6033 - loss: 0.6589 - val_accuracy: 0.6121 - val_loss: 0.6543\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 373ms/step - accuracy: 0.6094 - loss: 0.6542 - val_accuracy: 0.6211 - val_loss: 0.6499\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 378ms/step - accuracy: 0.6041 - loss: 0.6555 - val_accuracy: 0.6054 - val_loss: 0.6543\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 382ms/step - accuracy: 0.6052 - loss: 0.6584 - val_accuracy: 0.5904 - val_loss: 0.6590\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 376ms/step - accuracy: 0.6095 - loss: 0.6554 - val_accuracy: 0.6217 - val_loss: 0.6503\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 370ms/step - accuracy: 0.6058 - loss: 0.6537 - val_accuracy: 0.6126 - val_loss: 0.6470\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 373ms/step - accuracy: 0.6144 - loss: 0.6524 - val_accuracy: 0.6125 - val_loss: 0.6568\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 372ms/step - accuracy: 0.6131 - loss: 0.6513 - val_accuracy: 0.6184 - val_loss: 0.6469\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 374ms/step - accuracy: 0.6148 - loss: 0.6499 - val_accuracy: 0.6198 - val_loss: 0.6477\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 376ms/step - accuracy: 0.6118 - loss: 0.6527 - val_accuracy: 0.6155 - val_loss: 0.6507\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 365ms/step - accuracy: 0.6078 - loss: 0.6529 - val_accuracy: 0.6266 - val_loss: 0.6472\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 370ms/step - accuracy: 0.6164 - loss: 0.6517 - val_accuracy: 0.6234 - val_loss: 0.6489\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 376ms/step - accuracy: 0.6163 - loss: 0.6478 - val_accuracy: 0.6127 - val_loss: 0.6490\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 375ms/step - accuracy: 0.6173 - loss: 0.6485 - val_accuracy: 0.6256 - val_loss: 0.6455\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 274ms/step - accuracy: 0.6133 - loss: 0.6523 - val_accuracy: 0.6230 - val_loss: 0.6478\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 207ms/step - accuracy: 0.6157 - loss: 0.6501 - val_accuracy: 0.6097 - val_loss: 0.6566\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 172ms/step - accuracy: 0.6133 - loss: 0.6500 - val_accuracy: 0.6277 - val_loss: 0.6444\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 167ms/step - accuracy: 0.6151 - loss: 0.6513 - val_accuracy: 0.6084 - val_loss: 0.6481\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 165ms/step - accuracy: 0.6160 - loss: 0.6489 - val_accuracy: 0.6248 - val_loss: 0.6445\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train models using data generators\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Image model training\n",
    "image_train_generator = data_generator.flow(x_train_img, y_train_img, batch_size=batch_size)\n",
    "image_validation_generator = data_generator.flow(x_test_img, y_test_img, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    image_history = model_img.fit(image_train_generator, \n",
    "                                  steps_per_epoch=len(x_train_img) // batch_size,\n",
    "                                  validation_data=image_validation_generator,\n",
    "                                  validation_steps=len(x_test_img) // batch_size,\n",
    "                                  epochs=1)\n",
    "\n",
    "# Video model training\n",
    "video_train_generator = data_generator.flow(x_train_video, y_train_video, batch_size=batch_size)\n",
    "video_validation_generator = data_generator.flow(x_test_video, y_test_video, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    video_history = model_video.fit(video_train_generator, \n",
    "                                    steps_per_epoch=len(x_train_video) // batch_size,\n",
    "                                    validation_data=video_validation_generator,\n",
    "                                    validation_steps=len(x_test_video) // batch_size,\n",
    "                                    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08311508-c331-46db-9dfc-b5cf3572852d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Step 8: Save models\n",
    "model_img.save('resnet_image_classifier_model.h5')\n",
    "model_video.save('resnet_video_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4783bbd-3d8c-457a-a633-3ba2c60943e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
